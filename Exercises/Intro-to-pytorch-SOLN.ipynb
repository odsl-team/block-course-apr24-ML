{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1003ed-bebd-4fec-8148-cd27f0f5e3db",
   "metadata": {},
   "source": [
    "# Intro to Pytorch\n",
    "\n",
    "Yesterday, we coded up the forward pass and backward propagation _by scratch_. Today, we're going to use an automatic differentiation framework :) We had checked our manual gradients before in `jax` b/c the syntax is very transparent for these types of gradient checks, but we'll use `pytorch` for the rest of the block course because it's a great balance between ease of use for projects, while still having it be easy to dive back into the matrix / tensor manipulation code easily (ðŸ¥¸) if needed (ðŸ¤“).\n",
    "\n",
    "**Table of Contents**\n",
    "1. Build a simple MLP\n",
    "2. (?)\n",
    "3. Loss functions\n",
    "4. Softmax interlude\n",
    "5. Train the NN (with Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc85281-559d-49b4-a610-aff4eba54cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015da83a-047d-405d-ba18-47d1b646e2b0",
   "metadata": {},
   "source": [
    "**Pytorch Lecture notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf3f474-1456-4064-ad13-6701601c25e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# How to access the gradient of a tensor\n",
    "w = torch.tensor([0.1,  0.2,  2, 1, 0.1],\n",
    "                 requires_grad=True)\n",
    "print(w.grad) # test\n",
    "# Note this will get filled once we call `.backward` on the graph at some point in the computation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da81d062-50b3-4656-9491-5dc185682e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our new favorite fct <3 \n",
    "# torch.einsum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3c09b-c7ee-4ff4-885d-e9ae26b0c8d0",
   "metadata": {},
   "source": [
    "### 0) Load in our \"data generator\" (same as the last notebook, `Our-first-NN.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1e359e-0f57-4bf1-9122-7172e5df57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_true = [5, 4, -2, -0.7]\n",
    "\n",
    "def generate_data(N):\n",
    "    '''\n",
    "    Same function as yesterday\n",
    "    '''\n",
    "    x = np.random.uniform(low=-1, high=1, size=N)\n",
    "    \n",
    "    # y ~ N(mu=f(x), std=0.2)\n",
    "    mu = np.polyval(coeffs_true,x)\n",
    "    std = 0.2 * np.random.randn(N)\n",
    "    y = mu + std\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "def make_features(N, degree=4):\n",
    "    x,y = generate_data(N)\n",
    "    X = np.column_stack([x**i for i in reversed(range(degree+1))])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744f4109-ec7a-490b-bd15-256df3800002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (200, 5)\n",
      "y (200,)\n"
     ]
    }
   ],
   "source": [
    "N=200\n",
    "X_np,y_np = make_features(N)\n",
    "\n",
    "print('X',X_np.shape)\n",
    "print('y',y_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd03ffb-4078-446d-a2cd-1d2f26cfead7",
   "metadata": {},
   "source": [
    "Type case the np arrays to torch tensors ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8bd4fc6-abed-409d-b5ad-c19d46d7d10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([200, 5])\n",
      "y torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "N=200\n",
    "X = torch.tensor(X_np,dtype=torch.float32)\n",
    "y = torch.tensor(y_np,dtype=torch.float32)\n",
    "y = y[:,None] # want the output of y to match the output of v\n",
    "\n",
    "print('X',X.shape)\n",
    "print('y',y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a06757-989b-4425-84ef-c23449eb0d50",
   "metadata": {},
   "source": [
    "### 1) Build the simple MLP in `pytorch` that we've been playing with yesterday\n",
    "- Input ï¿¼$X \\in \\mathbb{R}^{N \\times d}$, d=5\n",
    "- NN with a single hidden layer, $H=16$ hidden units\n",
    "- ReLU nonlinearity\n",
    "- Output $y \\in \\mathbb{R}^N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6904547-26fd-4c2e-b11a-914e36b8408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "d = X.shape[1]\n",
    "H = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d03888-857f-4488-a40f-1e81393ac738",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Option 1: With Sequential\n",
    "'''\n",
    "f = nn.Sequential(nn.Linear(d,H), nn.ReLU(), nn.Linear(H,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65d1a2a7-c597-4dd9-a346-162c4a875b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the evaluation, does it have the shape you expect??\n",
    "f(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00a2d7c-9c2c-475a-9934-6c2f18c2dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Option 2: With the functional form\n",
    "'''\n",
    "\n",
    "class myMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myMLP, self).__init__()\n",
    "        self.lin1 = nn.Linear(d,H)\n",
    "        self.lin2 = nn.Linear(H,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        z = self.lin1(x)\n",
    "        h = nn.ReLU()(z)\n",
    "        y = self.lin2(h)\n",
    "        return y\n",
    "        \n",
    "\n",
    "ff = myMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7fa5e9-fe1e-43dd-8bff-b477b0069508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3954210-0016-4d71-88da-261c2acd6cd4",
   "metadata": {},
   "source": [
    "OK, it's great we have both implementations, but we'll just use `f` moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249f0b9-e570-466a-a625-b6c3a5962031",
   "metadata": {},
   "source": [
    "## 2: Mean Squared Error loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8759b-3712-4989-9e01-4da99b4f7ffd",
   "metadata": {},
   "source": [
    "Note, torch computes the compuation graph when we call `.backward`.\n",
    "\n",
    "Let's illustrate this w/ a linear model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f77d0159-4305-452e-9a35-c311c3efdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.,2.])\n",
    "w = torch.tensor([.2,.3],requires_grad=True)\n",
    "\n",
    "f_lin = w @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4603fa-27b6-43ed-9943-388239f5afb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20895838-6d83-4a87-b1a4-4922e53ece03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def7d018-01d0-4ac7-b096-c5647eac334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lin.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5dd1f12-9c13-4e85-95ac-38fe44759ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad) # is x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a62c76-464d-4286-9d7b-d62218ec6f4d",
   "metadata": {},
   "source": [
    "Another pytorch \"gotcha\": when you call .backward() multiple times... you _sum up the gradients_. \n",
    "\n",
    "What does this look like??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06fdc752-fb22-4926-8032-46f7d9609929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the ex above, a lin model with just two weights\n",
    "m = nn.Sequential(nn.Linear(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "845e0b2e-9c74-48a8-9dba-6de338f2780e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.3902, 0.1299]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.parameters().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9f57093-b9b2-4f72-a908-33b78e8719db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6206], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a63bf32d-f255-4067-85cd-758afb2d9aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 df/dw = tensor([[1., 2.]])\n",
      "Iter 1 df/dw = tensor([[2., 4.]])\n",
      "Iter 2 df/dw = tensor([[3., 6.]])\n",
      "Iter 3 df/dw = tensor([[4., 8.]])\n",
      "Iter 4 df/dw = tensor([[ 5., 10.]])\n",
      "Iter 5 df/dw = tensor([[ 6., 12.]])\n",
      "Iter 6 df/dw = tensor([[ 7., 14.]])\n",
      "Iter 7 df/dw = tensor([[ 8., 16.]])\n",
      "Iter 8 df/dw = tensor([[ 9., 18.]])\n",
      "Iter 9 df/dw = tensor([[10., 20.]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fx = m(x)\n",
    "    fx.backward()\n",
    "    print(f'Iter {i} df/dw =',m.parameters().__next__().grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8362e5-b33c-4571-b3ac-61d727b34491",
   "metadata": {},
   "source": [
    "**Fix:** Need to zero out the gradient b/w calling `.backward()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6865f926-3342-42ce-885c-e63977bb6b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 df/dw = tensor([[1., 2.]])\n",
      "Iter 1 df/dw = tensor([[1., 2.]])\n",
      "Iter 2 df/dw = tensor([[1., 2.]])\n",
      "Iter 3 df/dw = tensor([[1., 2.]])\n",
      "Iter 4 df/dw = tensor([[1., 2.]])\n",
      "Iter 5 df/dw = tensor([[1., 2.]])\n",
      "Iter 6 df/dw = tensor([[1., 2.]])\n",
      "Iter 7 df/dw = tensor([[1., 2.]])\n",
      "Iter 8 df/dw = tensor([[1., 2.]])\n",
      "Iter 9 df/dw = tensor([[1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fx = m(x)\n",
    "    m.zero_grad()\n",
    "    fx.backward()\n",
    "    print(f'Iter {i} df/dw =',m.parameters().__next__().grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf7ac40-532b-497b-b3c7-a797f13456d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e4fd61-42fd-426a-9611-d7a9f2035393",
   "metadata": {},
   "source": [
    "#### Task for you!\n",
    "\n",
    "Calculate the loss of the simple MLP `f` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc148c07-9413-42db-a92a-b1502c414d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9017, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expects input, target to be passed to the layer\n",
    "loss = nn.MSELoss()(f(X),y )\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b353e1ba-f4e1-41a9-bdd9-c5be0cb4b090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9017, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((f(X)-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8c1d64d-c587-43d1-9450-4ffb861a4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the computational graph\n",
    "f.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bad7738-09ea-4007-af2b-1c3ac17d53be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 torch.Size([16, 5])\n",
      "tensor([[-0.1322, -0.1241, -0.1698, -0.1363, -0.1666],\n",
      "        [ 0.0119, -0.0148,  0.0190, -0.0253,  0.0350],\n",
      "        [-0.0214,  0.0244, -0.0278,  0.0319, -0.0367],\n",
      "        [-0.1359, -0.1044, -0.1739, -0.1131, -0.1743],\n",
      "        [ 0.0133,  0.0167,  0.0202,  0.0219,  0.0062],\n",
      "        [ 0.0125, -0.0156,  0.0199, -0.0255,  0.0291],\n",
      "        [ 0.0035, -0.0061,  0.0104, -0.0162,  0.0143],\n",
      "        [-0.1083, -0.1201, -0.1330, -0.1440, -0.1406],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0434,  0.0333,  0.0555,  0.0361,  0.0556],\n",
      "        [-0.1152, -0.1216, -0.1284, -0.1359, -0.1439],\n",
      "        [ 0.0188, -0.0234,  0.0302, -0.0403,  0.0561],\n",
      "        [-0.0199,  0.0233, -0.0276,  0.0328, -0.0395],\n",
      "        [-0.0880, -0.0677, -0.1127, -0.0732, -0.1129],\n",
      "        [-0.0219,  0.0255, -0.0300,  0.0355, -0.0424],\n",
      "        [-0.1896, -0.2092, -0.2322, -0.2595, -0.2922]])\n",
      "b1 torch.Size([16])\n",
      "tensor([-0.1666,  0.0350, -0.0367, -0.1743,  0.0062,  0.0291,  0.0143, -0.1406,\n",
      "         0.0000,  0.0556, -0.1439,  0.0561, -0.0395, -0.1129, -0.0424, -0.2922])\n",
      "W2 torch.Size([1, 16])\n",
      "tensor([[-0.5352, -0.1474, -0.0247, -0.0971,  0.0212, -0.0630, -0.0019, -0.4951,\n",
      "          0.0000, -0.9066, -0.0468, -0.1571, -0.0200, -0.7799, -0.0176, -0.0261]])\n",
      "b2 torch.Size([1])\n",
      "tensor([-1.2199])\n"
     ]
    }
   ],
   "source": [
    "# Print and save it to a dictionary\n",
    "keys = ['W1','b1','W2','b2']\n",
    "\n",
    "grad_torch = {}\n",
    "\n",
    "for k, p in zip(keys,f.parameters()):\n",
    "    print(k,p.shape)\n",
    "    print(p.grad)\n",
    "\n",
    "    grad_torch[k] = p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ced8ac-fb08-4fe0-9cbb-df3d27f87076",
   "metadata": {},
   "source": [
    "#### Differentiable Detective\n",
    "\n",
    "We're now in a place where we can use the Auto Diff to check the computational graph solution for $\\nabla_{W1} \\mathcal{L}$, $\\nabla_{b1} \\mathcal{L}$,$\\nabla_{W2} \\mathcal{L}$, $\\nabla_{b2} \\mathcal{L}$.\n",
    "\n",
    "Note, getting $\\nabla_{W1} f$, $\\nabla_{b1} f$,$\\nabla_{W2} f$, $\\nabla_{b2} f$ is a little annoying in pytorch b/c it wants to calculate the gradient of a single scalar, and then NN output is an (N,1) array, where N is the number of examples.\n",
    "\n",
    "The code snippet below gets you the sample-wise gradient. For the scope of this lecture, it's not expected that you need to understand the details of this code snippet, just that you can use the output to check your worksheet calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb76b575-8bc4-4784-8ba7-cfec3c183b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the dict\n",
    "grad_dict_f = {k:[] for k in keys}\n",
    "\n",
    "# Loop over each example in the batch\n",
    "for i in range(N):\n",
    "    # Take the grad w/r.t. the example\n",
    "    # A.k.a, set up a computation graph for the example\n",
    "\n",
    "    # Warning! Need to zero out the gradients first!!\n",
    "    f.zero_grad()\n",
    "    \n",
    "    f(X)[i].backward()\n",
    "\n",
    "    # Append the gradients to the list\n",
    "    for k, p in zip(keys,f.parameters()):\n",
    "        grad_dict_f[k].append(p.grad)\n",
    "\n",
    "# concatenate the lists\n",
    "for k in keys:\n",
    "    grad_dict_f[k] = torch.stack(grad_dict_f[k],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dcf873-4799-445e-a20b-828ec900f119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69f6ba-a020-4876-ab64-5c710b3742f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fa2f0-d6f1-413c-8db1-0cea9354fe55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a972816c-2ebd-41c6-8f64-248bd76cf651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*(f(X)-y)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98d72acc-d515-4ae4-b947-07e2709dc058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 16, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_dict_f['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c17bcbeb-b88b-4f67-bd12-deac34a058a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 16, 5])\n"
     ]
    }
   ],
   "source": [
    "dl_dq = 2*(f(X)-y)\n",
    "dl_dw1_batch = dl_dq[...,None] * grad_dict_f['W1']\n",
    "print(dl_dw1_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02664522-19ad-4346-a4ab-192ebae0a99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1322, -0.1241, -0.1698, -0.1363, -0.1666],\n",
       "        [ 0.0119, -0.0148,  0.0190, -0.0253,  0.0350],\n",
       "        [-0.0214,  0.0244, -0.0278,  0.0319, -0.0367],\n",
       "        [-0.1359, -0.1044, -0.1739, -0.1131, -0.1743],\n",
       "        [ 0.0133,  0.0167,  0.0202,  0.0219,  0.0062],\n",
       "        [ 0.0125, -0.0156,  0.0199, -0.0255,  0.0291],\n",
       "        [ 0.0035, -0.0061,  0.0104, -0.0162,  0.0143],\n",
       "        [-0.1083, -0.1201, -0.1330, -0.1440, -0.1406],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0434,  0.0333,  0.0555,  0.0361,  0.0556],\n",
       "        [-0.1152, -0.1216, -0.1284, -0.1359, -0.1439],\n",
       "        [ 0.0188, -0.0234,  0.0302, -0.0403,  0.0561],\n",
       "        [-0.0199,  0.0233, -0.0276,  0.0328, -0.0395],\n",
       "        [-0.0880, -0.0677, -0.1127, -0.0732, -0.1129],\n",
       "        [-0.0219,  0.0255, -0.0300,  0.0355, -0.0424],\n",
       "        [-0.1896, -0.2092, -0.2322, -0.2595, -0.2922]],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_dw1 = torch.mean(dl_dw1_batch,axis=0)\n",
    "dl_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "706a53e4-6090-4620-a3a6-acb4382631c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1322, -0.1241, -0.1698, -0.1363, -0.1666],\n",
       "        [ 0.0119, -0.0148,  0.0190, -0.0253,  0.0350],\n",
       "        [-0.0214,  0.0244, -0.0278,  0.0319, -0.0367],\n",
       "        [-0.1359, -0.1044, -0.1739, -0.1131, -0.1743],\n",
       "        [ 0.0133,  0.0167,  0.0202,  0.0219,  0.0062],\n",
       "        [ 0.0125, -0.0156,  0.0199, -0.0255,  0.0291],\n",
       "        [ 0.0035, -0.0061,  0.0104, -0.0162,  0.0143],\n",
       "        [-0.1083, -0.1201, -0.1330, -0.1440, -0.1406],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0434,  0.0333,  0.0555,  0.0361,  0.0556],\n",
       "        [-0.1152, -0.1216, -0.1284, -0.1359, -0.1439],\n",
       "        [ 0.0188, -0.0234,  0.0302, -0.0403,  0.0561],\n",
       "        [-0.0199,  0.0233, -0.0276,  0.0328, -0.0395],\n",
       "        [-0.0880, -0.0677, -0.1127, -0.0732, -0.1129],\n",
       "        [-0.0219,  0.0255, -0.0300,  0.0355, -0.0424],\n",
       "        [-0.1896, -0.2092, -0.2322, -0.2595, -0.2922]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_torch['W1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67916c0a-ed0c-459e-b4a5-6b409595e510",
   "metadata": {},
   "source": [
    "Nice! visually, they look the same... let's check the rest of the examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c645944d-8f85-4b18-9ead-5e2d4976c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dw2 = torch.mean(dl_dq[...,None] * grad_dict_f['W2'], axis=0)\n",
    "\n",
    "dl_db1 = torch.mean(dl_dq * grad_dict_f['b1'], axis=0)\n",
    "dl_db2 = torch.mean(dl_dq * grad_dict_f['b2'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ba533cc-ef04-4f5a-8468-88d7813c819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for k, manual_grad in zip(keys, [dl_dw1, dl_db1, dl_dw2, dl_db2]):\n",
    "    print(torch.all(torch.isclose(manual_grad,grad_torch[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd31120-a602-4a7b-8590-26aad5facc25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
