{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1003ed-bebd-4fec-8148-cd27f0f5e3db",
   "metadata": {},
   "source": [
    "# Intro to Pytorch\n",
    "\n",
    "Yesterday, we coded up the forward pass and backward propagation _by scratch_. Today, we're going to use an automatic differentiation framework :) We had checked our manual gradients before in `jax` b/c the syntax is very transparent for these types of gradient checks, but we'll use `pytorch` for the rest of the block course because it's a great balance between ease of use for projects, while still having it be easy to dive back into the matrix / tensor manipulation code easily (ðŸ¥¸) if needed (ðŸ¤“).\n",
    "\n",
    "**Table of Contents**\n",
    "1. Build a simple MLP\n",
    "2. Mean Squared Error Loss\n",
    "3. Gradient with respect to the Loss check\n",
    "4. Train the NN (with Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc85281-559d-49b4-a610-aff4eba54cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3c09b-c7ee-4ff4-885d-e9ae26b0c8d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 0) Load in our \"data generator\" (same as the last notebook, `Our-first-NN.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e359e-0f57-4bf1-9122-7172e5df57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_true = [5, 4, -2, -0.7]\n",
    "\n",
    "def generate_data(N):\n",
    "    '''\n",
    "    Same function as yesterday\n",
    "    '''\n",
    "    x = np.random.uniform(low=-1, high=1, size=N)\n",
    "    \n",
    "    # y ~ N(mu=f(x), std=0.2)\n",
    "    mu = np.polyval(coeffs_true,x)\n",
    "    std = 0.2 * np.random.randn(N)\n",
    "    y = mu + std\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "def make_features(N, degree=4):\n",
    "    x,y = generate_data(N)\n",
    "    X = np.column_stack([x**i for i in reversed(range(degree+1))])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f4109-ec7a-490b-bd15-256df3800002",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "X_np,y_np = make_features(N)\n",
    "\n",
    "print('X',X_np.shape)\n",
    "print('y',y_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd03ffb-4078-446d-a2cd-1d2f26cfead7",
   "metadata": {},
   "source": [
    "Type case the np arrays to torch tensors ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd4fc6-abed-409d-b5ad-c19d46d7d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "X = torch.tensor(X_np,dtype=torch.float32)\n",
    "y = torch.tensor(y_np,dtype=torch.float32)\n",
    "y = y[:,None] # want the output of y to match the output of v\n",
    "\n",
    "print('X',X.shape)\n",
    "print('y',y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a06757-989b-4425-84ef-c23449eb0d50",
   "metadata": {},
   "source": [
    "### 1) Build the simple MLP in `pytorch` that we've been playing with yesterday\n",
    "- Input ï¿¼$X \\in \\mathbb{R}^{N \\times d}$, d=5\n",
    "- NN with a single hidden layer, $H=16$ hidden units\n",
    "- ReLU nonlinearity\n",
    "- Output $y \\in \\mathbb{R}^N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6904547-26fd-4c2e-b11a-914e36b8408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "d = X.shape[1]\n",
    "H = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d03888-857f-4488-a40f-1e81393ac738",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Your turn! Define a NN\n",
    "'''\n",
    "f ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1a2a7-c597-4dd9-a346-162c4a875b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the evaluation, does it have the shape you expect??\n",
    "f(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249f0b9-e570-466a-a625-b6c3a5962031",
   "metadata": {},
   "source": [
    "## 2: Mean Squared Error loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8759b-3712-4989-9e01-4da99b4f7ffd",
   "metadata": {},
   "source": [
    "Note, torch computes the compuation graph when we call `.backward`.\n",
    "\n",
    "Let's illustrate this w/ a linear model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d0159-4305-452e-9a35-c311c3efdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.,2.])\n",
    "w = torch.tensor([.2,.3],requires_grad=True)\n",
    "\n",
    "f_lin = w @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4603fa-27b6-43ed-9943-388239f5afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20895838-6d83-4a87-b1a4-4922e53ece03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7d018-01d0-4ac7-b096-c5647eac334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lin.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd1f12-9c13-4e85-95ac-38fe44759ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w.grad) # is x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a62c76-464d-4286-9d7b-d62218ec6f4d",
   "metadata": {},
   "source": [
    "Another pytorch \"gotcha\": when you call .backward() multiple times... you _sum up the gradients_. \n",
    "\n",
    "What does this look like??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fdc752-fb22-4926-8032-46f7d9609929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the ex above, a lin model with just two weights\n",
    "m = nn.Sequential(nn.Linear(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e0b2e-9c74-48a8-9dba-6de338f2780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.parameters().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f57093-b9b2-4f72-a908-33b78e8719db",
   "metadata": {},
   "outputs": [],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63bf32d-f255-4067-85cd-758afb2d9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    fx = m(x)\n",
    "    fx.backward()\n",
    "    print(f'Iter {i} df/dw =',m.parameters().__next__().grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8362e5-b33c-4571-b3ac-61d727b34491",
   "metadata": {},
   "source": [
    "**Fix:** Need to zero out the gradient b/w calling `.backward()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865f926-3342-42ce-885c-e63977bb6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    fx = m(x)\n",
    "    m.zero_grad()\n",
    "    fx.backward()\n",
    "    print(f'Iter {i} df/dw =',m.parameters().__next__().grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf7ac40-532b-497b-b3c7-a797f13456d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e4fd61-42fd-426a-9611-d7a9f2035393",
   "metadata": {},
   "source": [
    "#### Task for you!\n",
    "\n",
    "Calculate the loss of the simple MLP `f` defined above.\n",
    "\n",
    "Note, this final should the average over all $N=200$ of the examples, do you have the dimesntionality that you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc148c07-9413-42db-a92a-b1502c414d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1d64d-c587-43d1-9450-4ffb861a4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the computational graph\n",
    "f.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad7738-09ea-4007-af2b-1c3ac17d53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and save it to a dictionary\n",
    "keys = ['W1','b1','W2','b2']\n",
    "\n",
    "grad_torch = {}\n",
    "\n",
    "for k, p in zip(keys,f.parameters()):\n",
    "    print(k,p.shape)\n",
    "    print(p.grad)\n",
    "\n",
    "    grad_torch[k] = p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b5b08-eef5-4382-b80c-495f3b263efc",
   "metadata": {},
   "source": [
    "Nice!! We have $\\nabla_{W1} \\mathcal{L}$ now, just like we always wanted!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ced8ac-fb08-4fe0-9cbb-df3d27f87076",
   "metadata": {},
   "source": [
    "#### A.k.a, Differentiable Detective\n",
    "\n",
    "We're now in a place where we can use the Auto Diff to check the computational graph solution for $\\nabla_{W1} \\mathcal{L}$, $\\nabla_{b1} \\mathcal{L}$,$\\nabla_{W2} \\mathcal{L}$, $\\nabla_{b2} \\mathcal{L}$ we derived at the beginning of the lecture.\n",
    "\n",
    "Note, getting $\\nabla_{W1} f$, $\\nabla_{b1} f$,$\\nabla_{W2} f$, $\\nabla_{b2} f$ is a little annoying in pytorch b/c it wants to calculate the gradient of a single scalar, and then NN output is an (N,1) array, where N is the number of examples.\n",
    "\n",
    "The code snippet below gets you the sample-wise gradient. For the scope of this lecture, it's not expected that you need to understand the details of this code snippet, just that you can use the output to check your worksheet calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76b575-8bc4-4784-8ba7-cfec3c183b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the dict\n",
    "grad_dict_f = {k:[] for k in keys}\n",
    "\n",
    "# Loop over each example in the batch\n",
    "for i in range(N):\n",
    "    # Take the grad w/r.t. the example\n",
    "    # A.k.a, set up a computation graph for the example\n",
    "\n",
    "    # Warning! Need to zero out the gradients first!!\n",
    "    f.zero_grad()\n",
    "    \n",
    "    f(X)[i].backward()\n",
    "\n",
    "    # Append the gradients to the list\n",
    "    for k, p in zip(keys,f.parameters()):\n",
    "        grad_dict_f[k].append(p.grad)\n",
    "\n",
    "# concatenate the lists\n",
    "for k in keys:\n",
    "    grad_dict_f[k] = torch.stack(grad_dict_f[k],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fa2f0-d6f1-413c-8db1-0cea9354fe55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02664522-19ad-4346-a4ab-192ebae0a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dw1 = # your code here\n",
    "dl_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a53e4-6090-4620-a3a6-acb4382631c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_torch['W1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67916c0a-ed0c-459e-b4a5-6b409595e510",
   "metadata": {},
   "source": [
    "^ Above you should visually compare the gradients for the torch calc and the formula you circled this morning on your worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645944d-8f85-4b18-9ead-5e2d4976c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dw2 = # your code here\n",
    "\n",
    "dl_db1 = \n",
    "dl_db2 = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba533cc-ef04-4f5a-8468-88d7813c819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, manual_grad in zip(keys, [dl_dw1, dl_db1, dl_dw2, dl_db2]):\n",
    "    print(torch.all(torch.isclose(manual_grad,grad_torch[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd31120-a602-4a7b-8590-26aad5facc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5df38e50-b77b-4b05-a822-b6b2db13f590",
   "metadata": {},
   "source": [
    "#### 4. Train in pytorch\n",
    "\n",
    "Train you tiny MLP regression model on this polynomial dataset:\n",
    "- Use the Adam optimizer \n",
    "- Monitor a training and test dataset\n",
    "- If time permits, explore the dependence on the training dataset size\n",
    "    * Is our model underfitting or over fitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65f504-0d04-4e42-852a-40294125b7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
